{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "from glob import glob\n",
    "import librosa\n",
    "import librosa.display \n",
    "import IPython.display as ipd\n",
    "from itertools import cycle\n",
    "sns.set_theme(style=\"white\", palette=None)\n",
    "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "color_cycle = cycle(color_pal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_extract(filepath):\n",
    "   audio_files = glob(filepath)\n",
    "   return(audio_files)\n",
    "\n",
    "audio_files1 = audio_extract(r'RAVDESS_UNCLEAN\\*\\*.wav')\n",
    "audio_files2 = audio_extract(r'TESS Toronto emotional speech set data\\*\\*.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAVDESS_UNCLEAN\\\\Actor_01\\\\03-01-01-01-01-01-01.wav'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_files1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_DF_Creator(filelist):\n",
    "    df = pd.DataFrame()\n",
    "    result=np.array([])\n",
    "    for file in filelist:\n",
    "        y, sr = librosa.load(file)\n",
    "        y_trimmed, _ = librosa.effects.trim(y, top_db=30)\n",
    "        stft=np.abs(librosa.stft(y_trimmed))\n",
    "        mfccs=np.mean(librosa.feature.mfcc(y=y_trimmed, sr=sr, n_mfcc=40).T, axis=0)\n",
    "        mel=np.mean(librosa.feature.melspectrogram(y=y_trimmed, sr=sr).T,axis=0)\n",
    "        chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T,axis=0)\n",
    "        result = np.hstack(( mfccs, mel, chroma))\n",
    "        if len(df.columns)==0:\n",
    "            column_names = [f'col{i+1}' for i in range(len(result))]\n",
    "            df = pd.DataFrame(columns=column_names)        \n",
    "        df.loc[len(df)] = result\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = Feature_DF_Creator(audio_files1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for file in audio_files1:\n",
    "    lst.append(file.split(\"-\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"feeling\"] = lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>col9</th>\n",
       "      <th>col10</th>\n",
       "      <th>...</th>\n",
       "      <th>col172</th>\n",
       "      <th>col173</th>\n",
       "      <th>col174</th>\n",
       "      <th>col175</th>\n",
       "      <th>col176</th>\n",
       "      <th>col177</th>\n",
       "      <th>col178</th>\n",
       "      <th>col179</th>\n",
       "      <th>col180</th>\n",
       "      <th>feeling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-485.435760</td>\n",
       "      <td>125.547394</td>\n",
       "      <td>-9.674954</td>\n",
       "      <td>24.648430</td>\n",
       "      <td>14.435009</td>\n",
       "      <td>-5.135542</td>\n",
       "      <td>-9.597648</td>\n",
       "      <td>-12.427287</td>\n",
       "      <td>-28.499622</td>\n",
       "      <td>-9.909325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621136</td>\n",
       "      <td>0.624444</td>\n",
       "      <td>0.562191</td>\n",
       "      <td>0.547790</td>\n",
       "      <td>0.609514</td>\n",
       "      <td>0.658921</td>\n",
       "      <td>0.660492</td>\n",
       "      <td>0.619759</td>\n",
       "      <td>0.549749</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-482.768616</td>\n",
       "      <td>116.302711</td>\n",
       "      <td>-7.825343</td>\n",
       "      <td>29.099020</td>\n",
       "      <td>13.977192</td>\n",
       "      <td>-7.079821</td>\n",
       "      <td>-8.166739</td>\n",
       "      <td>-14.216775</td>\n",
       "      <td>-28.183737</td>\n",
       "      <td>-6.398925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544650</td>\n",
       "      <td>0.547016</td>\n",
       "      <td>0.585322</td>\n",
       "      <td>0.502808</td>\n",
       "      <td>0.565033</td>\n",
       "      <td>0.614017</td>\n",
       "      <td>0.648091</td>\n",
       "      <td>0.636803</td>\n",
       "      <td>0.570133</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-473.852509</td>\n",
       "      <td>124.038818</td>\n",
       "      <td>-6.438735</td>\n",
       "      <td>27.357557</td>\n",
       "      <td>6.484826</td>\n",
       "      <td>-2.441445</td>\n",
       "      <td>-8.439914</td>\n",
       "      <td>-14.846803</td>\n",
       "      <td>-25.888828</td>\n",
       "      <td>-10.471822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634776</td>\n",
       "      <td>0.617904</td>\n",
       "      <td>0.493865</td>\n",
       "      <td>0.472746</td>\n",
       "      <td>0.565979</td>\n",
       "      <td>0.653376</td>\n",
       "      <td>0.638636</td>\n",
       "      <td>0.518308</td>\n",
       "      <td>0.513652</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-471.156189</td>\n",
       "      <td>114.611877</td>\n",
       "      <td>-0.160490</td>\n",
       "      <td>28.123051</td>\n",
       "      <td>9.483885</td>\n",
       "      <td>0.481433</td>\n",
       "      <td>-8.060109</td>\n",
       "      <td>-17.652515</td>\n",
       "      <td>-24.048738</td>\n",
       "      <td>-8.849402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610664</td>\n",
       "      <td>0.576785</td>\n",
       "      <td>0.557635</td>\n",
       "      <td>0.578445</td>\n",
       "      <td>0.683943</td>\n",
       "      <td>0.657794</td>\n",
       "      <td>0.586186</td>\n",
       "      <td>0.575430</td>\n",
       "      <td>0.579479</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-537.455322</td>\n",
       "      <td>115.149658</td>\n",
       "      <td>-0.266134</td>\n",
       "      <td>27.281794</td>\n",
       "      <td>11.806399</td>\n",
       "      <td>-0.392789</td>\n",
       "      <td>-10.151818</td>\n",
       "      <td>-9.871162</td>\n",
       "      <td>-21.435678</td>\n",
       "      <td>-11.071152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610679</td>\n",
       "      <td>0.589172</td>\n",
       "      <td>0.578974</td>\n",
       "      <td>0.538881</td>\n",
       "      <td>0.589539</td>\n",
       "      <td>0.625496</td>\n",
       "      <td>0.632840</td>\n",
       "      <td>0.660166</td>\n",
       "      <td>0.590782</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         col1        col2      col3       col4       col5      col6  \\\n",
       "0 -485.435760  125.547394 -9.674954  24.648430  14.435009 -5.135542   \n",
       "1 -482.768616  116.302711 -7.825343  29.099020  13.977192 -7.079821   \n",
       "2 -473.852509  124.038818 -6.438735  27.357557   6.484826 -2.441445   \n",
       "3 -471.156189  114.611877 -0.160490  28.123051   9.483885  0.481433   \n",
       "4 -537.455322  115.149658 -0.266134  27.281794  11.806399 -0.392789   \n",
       "\n",
       "        col7       col8       col9      col10  ...    col172    col173  \\\n",
       "0  -9.597648 -12.427287 -28.499622  -9.909325  ...  0.621136  0.624444   \n",
       "1  -8.166739 -14.216775 -28.183737  -6.398925  ...  0.544650  0.547016   \n",
       "2  -8.439914 -14.846803 -25.888828 -10.471822  ...  0.634776  0.617904   \n",
       "3  -8.060109 -17.652515 -24.048738  -8.849402  ...  0.610664  0.576785   \n",
       "4 -10.151818  -9.871162 -21.435678 -11.071152  ...  0.610679  0.589172   \n",
       "\n",
       "     col174    col175    col176    col177    col178    col179    col180  \\\n",
       "0  0.562191  0.547790  0.609514  0.658921  0.660492  0.619759  0.549749   \n",
       "1  0.585322  0.502808  0.565033  0.614017  0.648091  0.636803  0.570133   \n",
       "2  0.493865  0.472746  0.565979  0.653376  0.638636  0.518308  0.513652   \n",
       "3  0.557635  0.578445  0.683943  0.657794  0.586186  0.575430  0.579479   \n",
       "4  0.578974  0.538881  0.589539  0.625496  0.632840  0.660166  0.590782   \n",
       "\n",
       "   feeling  \n",
       "0       01  \n",
       "1       01  \n",
       "2       01  \n",
       "3       01  \n",
       "4       02  \n",
       "\n",
       "[5 rows x 181 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "value_counts = df1['feeling'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the values and their counts\n",
    "plt.bar(value_counts.index, value_counts.values)\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Each Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emotion = {\"01\":\"neutral\", \"02\":\"calm\", \"03\":\"happy\", \"04\":\"sad\", \"05\":\"angry\", \"06\":\"fearful\", \"07\":\"disgust\", \"08\":\"surprised\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting target col into numeric\n",
    "df1[\"feeling\"] = df1[\"feeling\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[df1['feeling'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1344, 181)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_counts = df1['feeling'].value_counts()\n",
    "# Plot the values and their counts\n",
    "plt.bar(value_counts.index, value_counts.values)\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Each Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>col9</th>\n",
       "      <th>col10</th>\n",
       "      <th>...</th>\n",
       "      <th>col172</th>\n",
       "      <th>col173</th>\n",
       "      <th>col174</th>\n",
       "      <th>col175</th>\n",
       "      <th>col176</th>\n",
       "      <th>col177</th>\n",
       "      <th>col178</th>\n",
       "      <th>col179</th>\n",
       "      <th>col180</th>\n",
       "      <th>feeling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-537.455322</td>\n",
       "      <td>115.149658</td>\n",
       "      <td>-0.266134</td>\n",
       "      <td>27.281794</td>\n",
       "      <td>11.806399</td>\n",
       "      <td>-0.392789</td>\n",
       "      <td>-10.151818</td>\n",
       "      <td>-9.871162</td>\n",
       "      <td>-21.435678</td>\n",
       "      <td>-11.071152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610679</td>\n",
       "      <td>0.589172</td>\n",
       "      <td>0.578974</td>\n",
       "      <td>0.538881</td>\n",
       "      <td>0.589539</td>\n",
       "      <td>0.625496</td>\n",
       "      <td>0.632840</td>\n",
       "      <td>0.660166</td>\n",
       "      <td>0.590782</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-541.221741</td>\n",
       "      <td>126.537903</td>\n",
       "      <td>2.454986</td>\n",
       "      <td>20.882959</td>\n",
       "      <td>16.800463</td>\n",
       "      <td>0.455333</td>\n",
       "      <td>-10.365332</td>\n",
       "      <td>-10.118616</td>\n",
       "      <td>-16.742668</td>\n",
       "      <td>-10.085868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693676</td>\n",
       "      <td>0.586901</td>\n",
       "      <td>0.561591</td>\n",
       "      <td>0.626653</td>\n",
       "      <td>0.671709</td>\n",
       "      <td>0.649352</td>\n",
       "      <td>0.580083</td>\n",
       "      <td>0.610073</td>\n",
       "      <td>0.531500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-499.443176</td>\n",
       "      <td>123.253372</td>\n",
       "      <td>-5.768768</td>\n",
       "      <td>23.859962</td>\n",
       "      <td>7.796982</td>\n",
       "      <td>0.957703</td>\n",
       "      <td>-4.944527</td>\n",
       "      <td>-10.831729</td>\n",
       "      <td>-19.355633</td>\n",
       "      <td>-11.274094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645332</td>\n",
       "      <td>0.616189</td>\n",
       "      <td>0.559771</td>\n",
       "      <td>0.528220</td>\n",
       "      <td>0.547751</td>\n",
       "      <td>0.590485</td>\n",
       "      <td>0.618549</td>\n",
       "      <td>0.571405</td>\n",
       "      <td>0.550792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-496.596985</td>\n",
       "      <td>129.679184</td>\n",
       "      <td>-6.099539</td>\n",
       "      <td>23.197090</td>\n",
       "      <td>11.218100</td>\n",
       "      <td>-0.728725</td>\n",
       "      <td>-5.489161</td>\n",
       "      <td>-14.771072</td>\n",
       "      <td>-18.295977</td>\n",
       "      <td>-10.740245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586864</td>\n",
       "      <td>0.533162</td>\n",
       "      <td>0.484179</td>\n",
       "      <td>0.502631</td>\n",
       "      <td>0.559042</td>\n",
       "      <td>0.655913</td>\n",
       "      <td>0.629350</td>\n",
       "      <td>0.596757</td>\n",
       "      <td>0.491704</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-552.144043</td>\n",
       "      <td>126.035622</td>\n",
       "      <td>4.209912</td>\n",
       "      <td>24.098150</td>\n",
       "      <td>13.817362</td>\n",
       "      <td>0.534192</td>\n",
       "      <td>-9.881056</td>\n",
       "      <td>-6.894940</td>\n",
       "      <td>-13.667630</td>\n",
       "      <td>-10.371347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614366</td>\n",
       "      <td>0.605786</td>\n",
       "      <td>0.624982</td>\n",
       "      <td>0.645263</td>\n",
       "      <td>0.707637</td>\n",
       "      <td>0.630527</td>\n",
       "      <td>0.552646</td>\n",
       "      <td>0.595972</td>\n",
       "      <td>0.578731</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         col1        col2      col3       col4       col5      col6  \\\n",
       "4 -537.455322  115.149658 -0.266134  27.281794  11.806399 -0.392789   \n",
       "5 -541.221741  126.537903  2.454986  20.882959  16.800463  0.455333   \n",
       "6 -499.443176  123.253372 -5.768768  23.859962   7.796982  0.957703   \n",
       "7 -496.596985  129.679184 -6.099539  23.197090  11.218100 -0.728725   \n",
       "8 -552.144043  126.035622  4.209912  24.098150  13.817362  0.534192   \n",
       "\n",
       "        col7       col8       col9      col10  ...    col172    col173  \\\n",
       "4 -10.151818  -9.871162 -21.435678 -11.071152  ...  0.610679  0.589172   \n",
       "5 -10.365332 -10.118616 -16.742668 -10.085868  ...  0.693676  0.586901   \n",
       "6  -4.944527 -10.831729 -19.355633 -11.274094  ...  0.645332  0.616189   \n",
       "7  -5.489161 -14.771072 -18.295977 -10.740245  ...  0.586864  0.533162   \n",
       "8  -9.881056  -6.894940 -13.667630 -10.371347  ...  0.614366  0.605786   \n",
       "\n",
       "     col174    col175    col176    col177    col178    col179    col180  \\\n",
       "4  0.578974  0.538881  0.589539  0.625496  0.632840  0.660166  0.590782   \n",
       "5  0.561591  0.626653  0.671709  0.649352  0.580083  0.610073  0.531500   \n",
       "6  0.559771  0.528220  0.547751  0.590485  0.618549  0.571405  0.550792   \n",
       "7  0.484179  0.502631  0.559042  0.655913  0.629350  0.596757  0.491704   \n",
       "8  0.624982  0.645263  0.707637  0.630527  0.552646  0.595972  0.578731   \n",
       "\n",
       "   feeling  \n",
       "4        2  \n",
       "5        2  \n",
       "6        2  \n",
       "7        2  \n",
       "8        2  \n",
       "\n",
       "[5 rows x 181 columns]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"feeling\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def train_test_splitting(dataframe, targetname):\n",
    "    X = dataframe.drop(columns=[targetname])\n",
    "    y = dataframe[targetname]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"feeling\"\n",
    "X_train, X_test, y_train, y_test = train_test_splitting(df1, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feeling\n",
       "6    163\n",
       "5    157\n",
       "2    155\n",
       "8    153\n",
       "7    150\n",
       "3    149\n",
       "4    148\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>col9</th>\n",
       "      <th>col10</th>\n",
       "      <th>...</th>\n",
       "      <th>col171</th>\n",
       "      <th>col172</th>\n",
       "      <th>col173</th>\n",
       "      <th>col174</th>\n",
       "      <th>col175</th>\n",
       "      <th>col176</th>\n",
       "      <th>col177</th>\n",
       "      <th>col178</th>\n",
       "      <th>col179</th>\n",
       "      <th>col180</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>-528.048889</td>\n",
       "      <td>85.506065</td>\n",
       "      <td>8.848557</td>\n",
       "      <td>21.549112</td>\n",
       "      <td>-2.730648</td>\n",
       "      <td>-2.422154</td>\n",
       "      <td>-2.558138</td>\n",
       "      <td>-25.455044</td>\n",
       "      <td>-8.692566</td>\n",
       "      <td>-7.621813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426921</td>\n",
       "      <td>0.410471</td>\n",
       "      <td>0.502617</td>\n",
       "      <td>0.587728</td>\n",
       "      <td>0.573630</td>\n",
       "      <td>0.706195</td>\n",
       "      <td>0.693216</td>\n",
       "      <td>0.621626</td>\n",
       "      <td>0.563032</td>\n",
       "      <td>0.463683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>-302.241760</td>\n",
       "      <td>61.269974</td>\n",
       "      <td>-26.594875</td>\n",
       "      <td>0.839910</td>\n",
       "      <td>-13.823068</td>\n",
       "      <td>-28.064575</td>\n",
       "      <td>-16.838957</td>\n",
       "      <td>-27.129690</td>\n",
       "      <td>-21.079233</td>\n",
       "      <td>-6.779252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438385</td>\n",
       "      <td>0.479439</td>\n",
       "      <td>0.525163</td>\n",
       "      <td>0.592745</td>\n",
       "      <td>0.418174</td>\n",
       "      <td>0.333015</td>\n",
       "      <td>0.379118</td>\n",
       "      <td>0.441003</td>\n",
       "      <td>0.447827</td>\n",
       "      <td>0.380899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>-472.541718</td>\n",
       "      <td>93.215523</td>\n",
       "      <td>-2.390948</td>\n",
       "      <td>17.364607</td>\n",
       "      <td>9.029876</td>\n",
       "      <td>-17.478127</td>\n",
       "      <td>-23.993280</td>\n",
       "      <td>-17.537066</td>\n",
       "      <td>-19.044020</td>\n",
       "      <td>-12.396148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508848</td>\n",
       "      <td>0.580890</td>\n",
       "      <td>0.581301</td>\n",
       "      <td>0.595752</td>\n",
       "      <td>0.578879</td>\n",
       "      <td>0.608542</td>\n",
       "      <td>0.497990</td>\n",
       "      <td>0.341173</td>\n",
       "      <td>0.320068</td>\n",
       "      <td>0.414875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>-413.382843</td>\n",
       "      <td>88.397560</td>\n",
       "      <td>-39.533028</td>\n",
       "      <td>10.490053</td>\n",
       "      <td>-4.329689</td>\n",
       "      <td>-20.413452</td>\n",
       "      <td>-6.067649</td>\n",
       "      <td>-23.145451</td>\n",
       "      <td>-15.090004</td>\n",
       "      <td>-2.072168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478548</td>\n",
       "      <td>0.626476</td>\n",
       "      <td>0.602070</td>\n",
       "      <td>0.506595</td>\n",
       "      <td>0.446798</td>\n",
       "      <td>0.498299</td>\n",
       "      <td>0.579426</td>\n",
       "      <td>0.556527</td>\n",
       "      <td>0.540898</td>\n",
       "      <td>0.435067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>-303.514862</td>\n",
       "      <td>30.149023</td>\n",
       "      <td>-63.906593</td>\n",
       "      <td>-19.087793</td>\n",
       "      <td>-35.203648</td>\n",
       "      <td>-40.411041</td>\n",
       "      <td>-15.543701</td>\n",
       "      <td>-39.447006</td>\n",
       "      <td>-8.756002</td>\n",
       "      <td>-11.013502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393504</td>\n",
       "      <td>0.340688</td>\n",
       "      <td>0.361494</td>\n",
       "      <td>0.389748</td>\n",
       "      <td>0.349932</td>\n",
       "      <td>0.441435</td>\n",
       "      <td>0.439185</td>\n",
       "      <td>0.483950</td>\n",
       "      <td>0.558877</td>\n",
       "      <td>0.579138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           col1       col2       col3       col4       col5       col6  \\\n",
       "426 -528.048889  85.506065   8.848557  21.549112  -2.730648  -2.422154   \n",
       "582 -302.241760  61.269974 -26.594875   0.839910 -13.823068 -28.064575   \n",
       "398 -472.541718  93.215523  -2.390948  17.364607   9.029876 -17.478127   \n",
       "672 -413.382843  88.397560 -39.533028  10.490053  -4.329689 -20.413452   \n",
       "823 -303.514862  30.149023 -63.906593 -19.087793 -35.203648 -40.411041   \n",
       "\n",
       "          col7       col8       col9      col10  ...    col171    col172  \\\n",
       "426  -2.558138 -25.455044  -8.692566  -7.621813  ...  0.426921  0.410471   \n",
       "582 -16.838957 -27.129690 -21.079233  -6.779252  ...  0.438385  0.479439   \n",
       "398 -23.993280 -17.537066 -19.044020 -12.396148  ...  0.508848  0.580890   \n",
       "672  -6.067649 -23.145451 -15.090004  -2.072168  ...  0.478548  0.626476   \n",
       "823 -15.543701 -39.447006  -8.756002 -11.013502  ...  0.393504  0.340688   \n",
       "\n",
       "       col173    col174    col175    col176    col177    col178    col179  \\\n",
       "426  0.502617  0.587728  0.573630  0.706195  0.693216  0.621626  0.563032   \n",
       "582  0.525163  0.592745  0.418174  0.333015  0.379118  0.441003  0.447827   \n",
       "398  0.581301  0.595752  0.578879  0.608542  0.497990  0.341173  0.320068   \n",
       "672  0.602070  0.506595  0.446798  0.498299  0.579426  0.556527  0.540898   \n",
       "823  0.361494  0.389748  0.349932  0.441435  0.439185  0.483950  0.558877   \n",
       "\n",
       "       col180  \n",
       "426  0.463683  \n",
       "582  0.380899  \n",
       "398  0.414875  \n",
       "672  0.435067  \n",
       "823  0.579138  \n",
       "\n",
       "[5 rows x 180 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[260], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_scaled\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25005433"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Assuming X_train is currently of shape (num_samples, num_features)\n",
    "\n",
    "# Reshape to add a third dimension (timesteps)\n",
    "\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train.shape[0], -1, X_train.shape[1]))\n",
    "\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test.shape[0], -1, X_test.shape[1]))\n",
    "\n",
    "# Now, X_train_reshaped should have the shape (num_samples, num_features, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.8490344 , -0.1592921 ,  0.30613023, ..., -0.73087174,\n",
       "         -1.0680901 , -0.3182753 ]],\n",
       "\n",
       "       [[ 1.1960708 , -0.38538605, -1.416395  , ..., -1.7408222 ,\n",
       "         -2.2101276 , -1.8066593 ]],\n",
       "\n",
       "       [[ 1.5348861 , -1.76316   , -1.2419    , ..., -1.7860271 ,\n",
       "         -1.8371286 , -0.60417265]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.3939123 , -1.6566112 , -0.67179906, ..., -1.3904599 ,\n",
       "         -0.05808382,  0.3074244 ]],\n",
       "\n",
       "       [[-0.8236009 ,  0.4400722 ,  0.2504975 , ..., -0.03853741,\n",
       "          0.29807076,  0.94395596]],\n",
       "\n",
       "       [[-1.1009988 , -0.4260301 ,  1.272936  , ...,  2.3661995 ,\n",
       "          1.2901777 ,  0.7612785 ]]], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples, time_steps, num_features = X_train_reshaped.shape\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 1, 64)             62720     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71169 (278.00 KB)\n",
      "Trainable params: 71169 (278.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten\n",
    "model = Sequential()\n",
    "model.add(LSTM(18, input_shape=(time_steps, num_features), return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 2/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 3/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 4/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 5/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 6/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 7/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 8/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 9/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 10/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 11/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 12/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 13/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 14/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 15/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 16/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 17/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 18/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 19/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 20/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 21/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 22/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 23/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 24/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 25/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 26/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 27/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 28/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 29/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 30/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 31/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 32/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 33/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 34/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 35/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 36/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 37/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 38/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 39/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 40/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 41/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 42/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 43/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 44/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 45/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 46/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 47/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 48/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 49/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 50/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 51/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 52/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 53/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 54/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 55/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 56/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 57/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 58/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 59/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 60/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 61/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 62/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 63/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 64/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 65/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 66/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 67/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 68/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 69/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 70/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 71/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 72/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 73/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 74/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 75/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 76/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 77/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 78/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 79/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 80/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 81/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 82/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 83/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 84/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 85/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 86/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 87/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 88/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 89/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 90/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 91/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 92/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 93/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 94/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 95/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 96/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 97/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 98/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 99/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 100/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 101/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 102/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 103/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 104/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 105/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 106/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 107/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 108/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 109/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 110/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 111/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 112/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 113/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 114/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 115/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 116/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 117/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 118/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 119/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 120/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 121/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 122/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 123/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 124/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 125/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 126/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 127/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 128/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 129/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 130/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 131/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 132/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 133/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 134/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 135/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 136/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 137/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 138/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 139/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 140/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 141/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 142/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 143/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 144/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 145/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 146/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 147/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 148/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 149/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 150/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 151/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 152/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 153/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 154/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 155/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 156/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 157/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 158/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 159/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 160/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 161/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 162/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 163/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 164/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 165/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 166/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 167/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 168/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 169/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 170/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 171/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 172/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 173/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 174/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 175/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 176/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 177/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 178/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 179/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 180/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 181/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 182/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 183/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 184/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 185/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 186/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 187/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 188/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 189/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 190/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 191/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 192/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 193/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 194/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 195/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 196/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 197/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 198/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 199/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 200/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 201/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 202/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 203/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 204/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 205/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 206/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 207/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 208/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 209/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 210/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 211/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 212/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 213/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 214/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 215/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 216/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 217/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 218/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 219/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 220/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 221/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 222/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 223/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 224/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 225/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 226/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 227/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 228/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 229/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 230/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 231/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 232/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 233/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 234/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 235/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 236/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 237/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 238/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 239/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 240/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 241/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 242/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 243/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 244/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 245/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 246/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 247/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 248/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 249/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 250/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 251/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 252/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 253/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 254/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 255/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 256/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 257/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 258/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 259/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 260/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 261/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 262/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 263/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 264/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 265/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 266/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 267/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 268/500\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 269/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 270/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 271/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 272/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 273/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 274/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 275/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 276/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 277/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 278/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 279/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 280/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 281/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 282/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 283/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 284/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 285/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 286/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 287/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 288/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 289/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 290/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 291/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 292/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 293/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 294/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 295/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 296/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 297/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 298/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 299/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 300/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 301/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 302/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 303/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 304/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 305/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 306/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 307/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 308/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 309/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 310/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 311/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 312/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 313/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 314/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 315/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 316/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 317/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 318/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 319/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 320/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 321/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 322/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 323/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 324/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 325/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 326/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 327/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 328/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 329/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 330/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 331/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 332/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 333/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 334/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 335/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 336/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 337/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 338/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 339/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 340/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 341/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 342/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 343/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 344/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 345/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 346/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 347/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 348/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 349/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 350/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 351/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 352/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 353/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 354/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 355/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 356/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 357/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 358/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 359/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 360/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 361/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 362/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 363/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 364/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 365/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 366/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 367/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 368/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 369/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 370/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 371/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 372/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 373/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 374/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 375/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 376/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 377/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 378/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 379/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 380/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 381/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 382/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 383/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 384/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 385/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 386/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 387/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 388/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 389/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 390/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 391/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 392/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 393/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 394/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 395/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 396/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 397/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 398/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 399/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 400/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 401/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 402/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 403/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 404/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 405/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 406/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 407/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 408/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 409/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 410/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 411/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 412/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 413/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 414/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 415/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 416/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 417/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 418/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 419/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 420/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 421/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 422/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 423/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 424/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 425/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 426/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 427/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 428/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 429/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 430/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 431/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 432/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 433/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 434/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 435/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 436/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 437/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 438/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 439/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 440/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 441/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 442/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 443/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 444/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 445/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 446/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 447/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 448/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 449/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 450/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 451/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 452/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 453/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 454/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 455/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 456/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 457/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 458/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 459/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 460/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 461/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 462/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 463/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 464/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 465/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 466/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 467/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 468/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 469/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 470/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 471/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 472/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 473/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 474/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 475/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 476/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 477/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 478/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 479/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 480/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 481/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 482/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 483/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 484/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 485/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 486/500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 487/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 488/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 489/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 490/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 491/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 492/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 493/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 494/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 495/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 496/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 497/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 498/500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 499/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n",
      "Epoch 500/500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.1285 - val_loss: 0.0000e+00 - val_accuracy: 0.1528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17a12d525d0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fitting the model with the reshaped data\n",
    "model.fit(X_train_reshaped, y_train_encoded, epochs=100, batch_size=18, validation_data=(X_test_reshaped, y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 33ms/step - loss: 1.5503 - accuracy: 0.3958\n",
      "Test Loss: 1.550295352935791, Test Accuracy: 0.3958333432674408\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 1s 24ms/step - loss: 1.5071 - accuracy: 0.3854\n",
      "Test Loss: 1.5070931911468506, Test Accuracy: 0.3854166567325592\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_reshaped, y_train_encoded)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feeling\n",
       "02    192\n",
       "03    192\n",
       "04    192\n",
       "05    192\n",
       "06    192\n",
       "07    192\n",
       "08    192\n",
       "01     96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[target].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.drop(\"feeling\",axis=1)\n",
    "y = df1[\"feeling\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.53455585,  1.7736765 ,  0.23835878, ...,  1.0768518 ,\n",
       "         0.6776464 , -0.01726022],\n",
       "       [-0.50965214,  1.4032892 ,  0.3316039 , ...,  0.9402695 ,\n",
       "         0.87117773,  0.21932596],\n",
       "       [-0.42640048,  1.7132356 ,  0.40150744, ...,  0.83614033,\n",
       "        -0.4742772 , -0.4362226 ],\n",
       "       ...,\n",
       "       [ 0.6132438 , -0.94802856, -0.819019  , ..., -0.55200166,\n",
       "        -0.1311167 ,  0.35034034],\n",
       "       [ 0.8646828 , -1.9289322 ,  0.11124155, ..., -0.68078053,\n",
       "        -0.5925043 , -0.06941622],\n",
       "       [ 0.6247886 , -1.1587198 ,  0.475925  , ...,  0.47310504,\n",
       "         0.736138  ,  0.9773594 ]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(600,400,100, 50), learning_rate='adaptive', max_iter=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-18 {color: black;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.01, batch_size=256,\n",
       "              hidden_layer_sizes=(600, 400, 100, 50), learning_rate=&#x27;adaptive&#x27;,\n",
       "              max_iter=480)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.01, batch_size=256,\n",
       "              hidden_layer_sizes=(600, 400, 100, 50), learning_rate=&#x27;adaptive&#x27;,\n",
       "              max_iter=480)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=256,\n",
       "              hidden_layer_sizes=(600, 400, 100, 50), learning_rate='adaptive',\n",
       "              max_iter=480)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.51%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.39%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (200,), (300,)],\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'batch_size': [64, 128, 256]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "270 fits failed out of a total of 810.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "170 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'activation' parameter of MLPClassifier must be a str among {'tanh', 'identity', 'relu', 'logistic'}. Got 'sigmoid' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "59 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'activation' parameter of MLPClassifier must be a str among {'logistic', 'tanh', 'relu', 'identity'}. Got 'sigmoid' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "41 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'activation' parameter of MLPClassifier must be a str among {'tanh', 'relu', 'identity', 'logistic'}. Got 'sigmoid' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.57395349 0.57395349 0.58511628 0.58511628 0.58790698 0.58790698\n",
      " 0.56651163 0.56651163 0.56465116 0.56465116 0.59255814 0.59255814\n",
      " 0.55627907 0.55627907 0.54697674 0.54697674 0.58046512 0.58046512\n",
      " 0.57116279 0.57116279 0.58697674 0.58697674 0.57116279 0.57116279\n",
      " 0.56465116 0.56465116 0.5627907  0.5627907  0.59627907 0.59627907\n",
      " 0.56372093 0.56372093 0.5627907  0.5627907  0.57953488 0.57953488\n",
      " 0.56465116 0.56465116 0.57860465 0.57860465 0.56186047 0.56186047\n",
      " 0.58232558 0.58232558 0.54976744 0.54976744 0.6        0.6\n",
      " 0.57953488 0.57953488 0.54604651 0.54604651 0.58604651 0.58604651\n",
      " 0.56       0.56       0.60837209 0.60837209 0.65023256 0.65023256\n",
      " 0.56651163 0.56651163 0.62232558 0.62232558 0.62883721 0.62883721\n",
      " 0.56465116 0.56465116 0.61860465 0.61860465 0.64372093 0.64372093\n",
      " 0.53860465 0.53860465 0.61674419 0.61674419 0.64       0.64\n",
      " 0.5627907  0.5627907  0.62883721 0.62883721 0.6455814  0.6455814\n",
      " 0.57209302 0.57209302 0.62418605 0.62418605 0.64651163 0.64651163\n",
      " 0.57674419 0.57674419 0.5944186  0.5944186  0.64651163 0.64651163\n",
      " 0.58511628 0.58511628 0.63348837 0.63348837 0.6344186  0.6344186\n",
      " 0.58046512 0.58046512 0.62232558 0.62232558 0.65953488 0.65953488\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-19 {color: black;}#sk-container-id-19 pre{padding: 0;}#sk-container-id-19 div.sk-toggleable {background-color: white;}#sk-container-id-19 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-19 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-19 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-19 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-19 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-19 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-19 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-19 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-19 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-19 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-19 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-19 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-19 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-19 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-19 div.sk-item {position: relative;z-index: 1;}#sk-container-id-19 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-19 div.sk-item::before, #sk-container-id-19 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-19 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-19 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-19 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-19 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-19 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-19 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-19 div.sk-label-container {text-align: center;}#sk-container-id-19 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-19 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-19\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=500, random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;, &#x27;sigmoid&#x27;],\n",
       "                         &#x27;alpha&#x27;: [0.0001, 0.001, 0.01],\n",
       "                         &#x27;batch_size&#x27;: [64, 128, 256],\n",
       "                         &#x27;hidden_layer_sizes&#x27;: [(100,), (200,), (300,)],\n",
       "                         &#x27;learning_rate&#x27;: [&#x27;constant&#x27;, &#x27;adaptive&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=500, random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;, &#x27;sigmoid&#x27;],\n",
       "                         &#x27;alpha&#x27;: [0.0001, 0.001, 0.01],\n",
       "                         &#x27;batch_size&#x27;: [64, 128, 256],\n",
       "                         &#x27;hidden_layer_sizes&#x27;: [(100,), (200,), (300,)],\n",
       "                         &#x27;learning_rate&#x27;: [&#x27;constant&#x27;, &#x27;adaptive&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=500, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=500, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=500, random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'activation': ['relu', 'tanh', 'sigmoid'],\n",
       "                         'alpha': [0.0001, 0.001, 0.01],\n",
       "                         'batch_size': [64, 128, 256],\n",
       "                         'hidden_layer_sizes': [(100,), (200,), (300,)],\n",
       "                         'learning_rate': ['constant', 'adaptive']})"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=500, random_state=42)\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'alpha': 0.01,\n",
       " 'batch_size': 256,\n",
       " 'hidden_layer_sizes': (300,),\n",
       " 'learning_rate': 'constant'}"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\basti\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-20 {color: black;}#sk-container-id-20 pre{padding: 0;}#sk-container-id-20 div.sk-toggleable {background-color: white;}#sk-container-id-20 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-20 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-20 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-20 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-20 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-20 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-20 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-20 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-20 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-20 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-20 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-20 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-20 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-20 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-20 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-20 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-20 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-20 div.sk-item {position: relative;z-index: 1;}#sk-container-id-20 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-20 div.sk-item::before, #sk-container-id-20 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-20 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-20 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-20 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-20 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-20 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-20 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-20 div.sk-label-container {text-align: center;}#sk-container-id-20 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-20 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-20\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.01, batch_size=256,\n",
       "              hidden_layer_sizes=(300,), max_iter=500, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.01, batch_size=256,\n",
       "              hidden_layer_sizes=(300,), max_iter=500, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.01, batch_size=256,\n",
       "              hidden_layer_sizes=(300,), max_iter=500, random_state=42)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = MLPClassifier(**best_params, max_iter=500, random_state=42)\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 67.29%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "accuracy = final_model.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "accuracy = final_model.score(X_train, y_train)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MLPClassifier' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[277], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MLPClassifier' object has no attribute 'summary'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
